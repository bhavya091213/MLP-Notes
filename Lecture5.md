Lecture5.md

# Lecture 5 - Linear Regression II
---
## Agenda:
1. Modeling to learning
   1. MLE and MAP
   2. L(w) = MMSE + $\lambda ||w||$
     * *(This equation appears to represent a loss function (L) that is a combination of Mean Squared Error (MMSE) and a regularization term, where $||w||$ is likely the L2 norm of the weight vector w, and $\lambda$ is a regularization parameter.)*
3. Convex Optimization Theory
   1. Necessary and sufficient condition of optimality
   2. Equality constraint problem
   3. Inequality constraint problem
   4. Three interpretations of MMSE with regularization
     * *(MMSE typically refers to Minimum Mean Squared Error, a criterion for estimating a variable. When used with regularization, it suggests a method to improve the generalization of a model by penalizing complex weight vectors.)*
---
## Recap
(Happened on blackboard)